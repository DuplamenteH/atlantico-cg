{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp39-cp39-win_amd64.whl (438.0 MB)\n",
      "Requirement already satisfied: gensim in c:\\users\\mathe\\anaconda3\\envs\\nlp\\lib\\site-packages (4.2.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.46.1-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mathe\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mathe\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.25.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.6.0-cp39-cp39-win_amd64.whl (2.8 MB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-14.0.1-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\mathe\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (1.22.3)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Using cached typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\users\\mathe\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Using cached protobuf-3.20.1-cp39-cp39-win_amd64.whl (904 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp39-cp39-win_amd64.whl (35 kB)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\mathe\\anaconda3\\envs\\nlp\\lib\\site-packages (from gensim) (1.8.0)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\mathe\\anaconda3\\envs\\nlp\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\mathe\\anaconda3\\envs\\nlp\\lib\\site-packages (from gensim) (6.0.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mathe\\anaconda3\\envs\\nlp\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\mathe\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Using cached importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.8.0-py3-none-any.whl (5.4 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mathe\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mathe\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mathe\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mathe\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=08a73dc6d627c1884a33d65c5ea9fce35da01c7c80b8021066b4eb552e4cd6b2\n",
      "  Stored in directory: c:\\users\\mathe\\appdata\\local\\pip\\cache\\wheels\\b6\\0d\\90\\0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, zipp, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.46.1 h5py-3.6.0 importlib-metadata-4.11.3 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.20.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.25.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 typing-extensions-4.2.0 werkzeug-2.1.2 wrapt-1.14.1 zipp-3.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_csv(\"socialmedia_relevant_cols_clean.csv\") \n",
    "questions.columns=['text', 'choose_one', 'class_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpando dados:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text(df, text_field):    \n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")    \n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")    \n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")    \n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")    \n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")    \n",
    "    df[text_field] = df[text_field].str.lower()    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions = standardize_text(questions, \"text\") \n",
    "clean_questions.to_csv(\"clean_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions.groupby(\"class_label\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quebrando os dados<p/>\n",
    "Agora que temos os dados limpos, vamos transformá-los para que o modelo possa entender.<p/>\n",
    " Logo:\n",
    "\n",
    "* Quebrar as sentenças em listas de palavras separadas;\n",
    "* Dividir os dados para treinamento e teste do modelo;\n",
    "* Inspecionar os dados novamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer \n",
    "# Método de quebra dos dados \n",
    "tokenizer = RegexpTokenizer(r'\\w+') \n",
    "# Gerando listas de sentenças quebradas \n",
    "clean_questions[\"tokens\"] = clean_questions[\"text\"].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primeiras linhas \n",
    "clean_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Últimas linhas \n",
    "clean_questions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecioanndo novamente os dados \n",
    "all_words = [word for tokens in clean_questions[\"tokens\"] for word in tokens] \n",
    "sentence_lengths = [len(tokens) for tokens in clean_questions[\"tokens\"]] \n",
    "VOCAB = sorted(list(set(all_words))) \n",
    "print(\"%s Quantidade total de palavras, com um vocabulario de %s\" % (len(all_words), len(VOCAB))) print(\"Tamanho máximo de uma sentença %s\" % max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribuilção das sentenças por quantidade de palavras \n",
    "fig = plt.figure(figsize=(10, 10)) \n",
    "plt.xlabel('Tamanho da setença') \n",
    "plt.ylabel('Número de sentenças') \n",
    "plt.hist(sentence_lengths) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construindo bag-of-words com count-vectorizer \n",
    "count_vectorizer = CountVectorizer() \n",
    "# Separando o texto da variável alvo \n",
    "list_corpus = clean_questions[\"text\"].tolist() \n",
    "list_labels = clean_questions[\"class_label\"].tolist() \n",
    "# Construindo exemplos para treinamento e teste \n",
    "X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, random_state=40)                                                           \n",
    "# Aprendendo o vacabulário e contabilizando os termos para teste \n",
    "X_train_counts = count_vectorizer.fit_transform(X_train) \n",
    "# Contabilizando os termos no vocabulário aprendido para treino \n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizando o Vocabulário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método para plotar o gráfico com a distribuição do vocabulário por variável alvo \n",
    "def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n",
    "        lsa = TruncatedSVD(n_components=2)        \n",
    "        lsa.fit(test_data)        \n",
    "        lsa_scores = lsa.transform(test_data)        \n",
    "        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}        \n",
    "        color_column = [color_mapper[label] for label in test_labels]        \n",
    "        colors = ['orange','blue','blue']        \n",
    "        if plot:            \n",
    "           plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "           red_patch = mpatches.Patch(color='orange', label='Irrelevant')            \n",
    "           green_patch = mpatches.Patch(color='blue', label='Disaster')           \n",
    "           plt.legend(handles=[red_patch, green_patch], prop={'size': 20})            \n",
    "           fig = plt.figure(figsize=(10, 10))          \n",
    "           plot_LSA(X_train_counts, y_train) \n",
    "           plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treiando o Classificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando configurações no modelo \n",
    "clf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg',multi_class='multinomial', n_jobs=-1, random_state=40) \n",
    "#Treinando o modelo com o vocabulário construindo e as variáveis alvo \n",
    "clf.fit(X_train_counts, y_train) \n",
    "#Realizando as predições para o conjunto de teste \n",
    "y_predicted_counts = clf.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliando o Classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4286059870.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [6]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Avaliando o Classificador\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Método para extração das métricas de avaliação usando a parte dos dados que separamos para teste. \n",
    "def get_metrics(y_test, y_predicted):      \n",
    "   # true positives / (true positives+false positives)    \n",
    "   precision = precision_score(y_test, y_predicted, pos_label=None,average='weighted')                 \n",
    "   # true positives / (true positives + false negatives)    \n",
    "   recall = recall_score(y_test, y_predicted, pos_label=None,average='weighted')        \n",
    "   # harmonic mean of precision and recall    \n",
    "   f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')        \n",
    "   # true positives + true negatives/ total    \n",
    "   accuracy = accuracy_score(y_test, y_predicted)    \n",
    "   return accuracy, precision, recall, f1 \n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts) \n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método para plotar a matriz de confusão. \n",
    "def plot_confusion_matrix(cm, classes,normalize=False, title='Confusion matrix', cmap=plt.cm.winter): \n",
    "        if normalize:        \n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]    \n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)    \n",
    "        plt.title(title, fontsize=30)    \n",
    "        plt.colorbar()    \n",
    "        tick_marks = np.arange(len(classes))    \n",
    "        plt.xticks(tick_marks, classes, fontsize=20)    \n",
    "        plt.yticks(tick_marks, classes, fontsize=20)        \n",
    "        fmt = '.2f' if normalize else 'd'    \n",
    "        thresh = cm.max() / 2.     \n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):        \n",
    "            plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\",color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n",
    "            plt.tight_layout()   \n",
    "            plt.ylabel('True label', fontsize=30)    \n",
    "            plt.xlabel('Predicted label', fontsize=30)     \n",
    "        return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_predicted_counts) \n",
    "fig = plt.figure(figsize=(9, 9)) \n",
    "plot = plot_confusion_matrix(cm, classes=['Irrelevant','Disaster','Unsure'], normalize=False, title='Confusion matrix') \n",
    "plt.show() \n",
    "print(\"Count Vectorizer confusion matrix\") \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método para identificação das features mais importantes na tomada de decisão. \n",
    "def get_most_important_features(vectorizer, model, n=5):    \n",
    "    index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}        \n",
    "    # loop for each class    \n",
    "    classes ={}    \n",
    "    for class_index in range(model.coef_.shape[0]):        \n",
    "        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]        \n",
    "        sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)        \n",
    "        tops = sorted(sorted_coeff[:n], key = lambda x : x[0])        \n",
    "        bottom = sorted_coeff[-n:]        \n",
    "        classes[class_index] = {'tops':tops,'bottom':bottom}    \n",
    "    return classes \n",
    "importance = get_most_important_features(count_vectorizer, clf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método para plotar as featrues mais importantes para cada variável alvo. \n",
    "def plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):    \n",
    "    y_pos = np.arange(len(top_words))    \n",
    "    top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]    \n",
    "    top_pairs = sorted(top_pairs, key=lambda x: x[1])        \n",
    "    bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]    \n",
    "    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)        \n",
    "    top_words = [a[0] for a in top_pairs]    \n",
    "    top_scores = [a[1] for a in top_pairs]        \n",
    "    bottom_words = [a[0] for a in bottom_pairs]    \n",
    "    bottom_scores = [a[1] for a in bottom_pairs]        \n",
    "    fig = plt.figure(figsize=(10, 10))      \n",
    "    plt.subplot(121)    \n",
    "    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)    \n",
    "    plt.title('Irrelevant', fontsize=20)    \n",
    "    plt.yticks(y_pos, bottom_words, fontsize=14)    \n",
    "    plt.suptitle('Key words', fontsize=16)    \n",
    "    plt.xlabel('Importance', fontsize=20)        \n",
    "    plt.subplot(122)    \n",
    "    plt.barh(y_pos,top_scores, align='center', alpha=0.5)    \n",
    "    plt.title('Disaster', fontsize=20)    \n",
    "    plt.yticks(y_pos, top_words, fontsize=14)    \n",
    "    plt.suptitle(name, fontsize=16)    \n",
    "    plt.xlabel('Importance', fontsize=20)        \n",
    "    plt.subplots_adjust(wspace=0.8)    \n",
    "    plt.show() \n",
    "top_scores = [a[0] for a in importance[1]['tops']] \n",
    "top_words = [a[1] for a in importance[1]['tops']] \n",
    "bottom_scores = [a[0] for a in importance[1]['bottom']] \n",
    "bottom_words = [a[1] for a in importance[1]['bottom']] \n",
    "plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construindo bag-of-words com count-vectorizer \n",
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "# Aprendendo o vacabulário e contabilizando os termos para teste \n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "# Contabilizando os termos no vocabulário aprendido para treino \n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test) \n",
    "# Plotando a distribuição do vocabulário \n",
    "fig = plt.figure(figsize=(10, 10))          \n",
    "plot_LSA(X_train_tfidf, y_train) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tfidf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', multi_class='multinomial', n_jobs=-1, random_state=40) \n",
    "#Treinando o modelo com o vocabulário construindo e as variáveis alvo \n",
    "clf_tfidf.fit(X_train_tfidf, y_train) \n",
    "y_predicted_tfidf = clf_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tfidf, precision_tfidf, recall_tfidf, f1_tfidf = get_metrics(y_test, y_predicted_tfidf) \n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_tfidf, precision_tfidf,recall_tfidf, f1_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = confusion_matrix(y_test, y_predicted_tfidf) \n",
    "fig = plt.figure(figsize=(9, 9)) \n",
    "plot = plot_confusion_matrix(cm2, classes=['Irrelevant','Disaster','Unsure'], normalize=False, title='Confusion matrix') \n",
    "plt.show() \n",
    "print(\"TFIDF confusion matrix\") \n",
    "print(cm2) \n",
    "print(\"BoW confusion matrix\") \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_tfidf = get_most_important_features(tfidf_vectorizer, clf_tfidf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scores = [a[0] for a in importance_tfidf[1]['tops']] \n",
    "top_words = [a[1] for a in importance_tfidf[1]['tops']] \n",
    "bottom_scores = [a[0] for a in importance_tfidf[1]['bottom']] \n",
    "bottom_words = [a[1] for a in importance_tfidf[1]['bottom']] \n",
    "plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treinando a matriz de relação com notícias do Google \n",
    "#O conjunto de dados para treinar a matriz pode ser acessado em: #https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit word2vec_path = \"GoogleNews-vectors-negative300.bin.gz\" \n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método para calcular a distância semântica entre as palavras \n",
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):    \n",
    "    if len(tokens_list)<1:        \n",
    "       return np.zeros(k)    \n",
    "    if generate_missing:        \n",
    "       vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]    \n",
    "    else:        \n",
    "       vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]    \n",
    "    length = len(vectorized)    \n",
    "    summed = np.sum(vectorized, axis=0)    \n",
    "    averaged = np.divide(summed, length)    \n",
    "    return averaged \n",
    "#Montagem do arquivo de treinamento contento a relação semântica entre as palavras \n",
    "def get_word2vec_embeddings(vectors, clean_questions, generate_missing=False):    \n",
    "    embeddings = clean_questions['tokens'].apply(lambda x: get_average_word2vec(x, vectors,generate_missing=generate_missing))    \n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_word2vec_embeddings(word2vec, clean_questions) \n",
    "X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(embeddings, list_labels,test_size=0.2, random_state=40) \n",
    "fig = plt.figure(figsize=(10, 10))          \n",
    "plot_LSA(embeddings, list_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_w2v = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg',multi_class='multinomial', random_state=40) \n",
    "clf_w2v.fit(X_train_word2vec, y_train_word2vec) \n",
    "y_predicted_word2vec = clf_w2v.predict(X_test_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_word2vec, precision_word2vec, recall_word2vec, f1_word2vec = get_metrics(y_test_word2vec, y_predicted_word2vec) \n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_word2vec, precision_word2vec,recall_word2vec, f1_word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_w2v = confusion_matrix(y_test_word2vec, y_predicted_word2vec) \n",
    "fig = plt.figure(figsize=(9, 9)) \n",
    "plot = plot_confusion_matrix(cm_w2v, classes=['Irrelevant','Disaster','Unsure'], normalize=False, title='Confusion matrix')\n",
    "plt.show() \n",
    "print(\"Word2Vec confusion matrix\") \n",
    "print(cm_w2v) \n",
    "print(\"TFIDF confusion matrix\") \n",
    "print(cm2) \n",
    "print(\"BoW confusion matrix\") \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenização e segmentação de sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza \n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize') \n",
    "doc = nlp('This is a test sentence for stanza. This is another sentence.') \n",
    "for i, sentence in enumerate(doc.sentences):     \n",
    "    print(f'====== Sentence {i+1} tokens =======')     \n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([sentence.text for sentence in doc.sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_no_ssplit=True)\n",
    "doc = nlp('This is a sentence.\\n\\nThis is a second. This is a third.')\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_pretokenized=True)\n",
    "doc = nlp([['This', 'is', 'token.ization', 'done', 'my', 'way!'], ['Sentence', 'split,', 'too!']])\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lematização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')\n",
    "doc = nlp('Barack Obama was born in Hawaii.')\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análise de sentimentos com NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples \n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples \n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json') \n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples \n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json') \n",
    "text = twitter_samples.strings('tweets.20150430-223406.json') \n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
    "print(tweet_tokens[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples \n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json') \n",
    "text = twitter_samples.strings('tweets.20150430-223406.json') \n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
    "print(tweet_tokens[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples \n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json') \n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json') \n",
    "text = twitter_samples.strings('tweets.20150430-223406.json') \n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
    " #print(tweet_tokens[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa 3 - Normalizando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []    \n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence \n",
    "print(lemmatize_sentence(tweet_tokens[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa 4 - Removendo o ruído dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "... \n",
    "import re, string \n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "    cleaned_tokens = []\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\'(?:%[0-9a-fA-F][0-9a-fA-F]))+\",'', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english') \n",
    "print(remove_noise(tweet_tokens[0], stop_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english') \n",
    "#print(remove_noise(tweet_tokens[0], stop_words)) \n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa 5 - Determinando a densidade da palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token \n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist \n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa 6 - Preparando dados para o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens) \n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "positive_dataset = [(tweet_dict, \"Positive\")for tweet_dict in positive_tokens_for_model] \n",
    "negative_dataset = [(tweet_dict, \"Negative\")for tweet_dict in negative_tokens_for_model] \n",
    "dataset = positive_dataset + negative_dataset \n",
    "random.shuffle(dataset) \n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa 7 - Criando e testando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data) \n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data)) \n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet)) \n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tweet = 'Congrats #SportStar on your 7th best goal from last season winning goal of the year :) #Baller #Topbin #oneofmanyworldies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b34f453616bd66a23ff7c24ab8c961ad1470f54ded2cb96b11b9d780b24cd9d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
